{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 1.069056      \n",
      "error 0.416177      \n",
      "error 0.396046      \n",
      "error 0.055331      \n",
      "error 0.018572      \n",
      "error 0.024463      \n",
      "error 0.011869      \n",
      "error 0.012942      \n",
      "error 0.010579      \n",
      "error 0.012479      \n",
      "[0, 0] -> [-0.05413047399935578]\n",
      "[0, 1] -> [0.9864543139793907]\n",
      "[1, 0] -> [0.9872136180950448]\n",
      "[1, 1] -> [0.08417522275687156]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pure Python implementation of a Back-Propagation Neural Network using the\n",
    "hyperbolic tangent as the sigmoid squashing function.\n",
    "Original Author: Neil Schemenauer <nas@arctrix.com>\n",
    "Modified Author: James Howard <james.w.howard@gmail.com>\n",
    "Modified to work for function regression and added option to use matplotlib\n",
    "to display regression networks.\n",
    "Code is placed in public domain.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# calculate a random number where:  a <= rand < b\n",
    "def rand(a, b):\n",
    "    return (b-a)*random.random() + a\n",
    "\n",
    "# Make a matrix (we could use NumPy to speed this up)\n",
    "def makeMatrix(I, J, fill=0.0):\n",
    "    m = []\n",
    "    for i in range(I):\n",
    "        m.append([fill]*J)\n",
    "    return m\n",
    "\n",
    "# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)\n",
    "def sigmoid(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    return 1.0 - y**2\n",
    "\n",
    "\n",
    "def plot(inputs, outputs, actual):\n",
    "    \"\"\"Plot a given function.  \n",
    "    \n",
    "    The actual function will be plotted with a line and the outputs with \n",
    "    points.  Useful for visualizing the error of the neural networks attempt \n",
    "    at function interpolation.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot\n",
    "    except:\n",
    "        raise (ImportError, \"matplotlib package not found.\")\n",
    "    fig = matplotlib.pyplot.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(inputs, actual, 'b-')\n",
    "    ax1.plot(inputs, outputs, 'r.')\n",
    "    matplotlib.pyplot.draw()\n",
    "    \n",
    "    \n",
    "\n",
    "class NN:\n",
    "    def __init__(self, ni, nh, no, regression = False):\n",
    "        \"\"\"NN constructor.\n",
    "        \n",
    "        ni, nh, no are the number of input, hidden and output nodes.\n",
    "        regression is used to determine if the Neural network will be trained \n",
    "        and used as a classifier or for function regression.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.regression = regression\n",
    "        \n",
    "        #Number of input, hidden and output nodes.\n",
    "        self.ni = ni  + 1 # +1 for bias node\n",
    "        self.nh = nh  + 1 # +1 for bias node\n",
    "        self.no = no\n",
    "\n",
    "        # activations for nodes\n",
    "        self.ai = [1.0]*self.ni\n",
    "        self.ah = [1.0]*self.nh\n",
    "        self.ao = [1.0]*self.no\n",
    "        \n",
    "        # create weights\n",
    "        self.wi = makeMatrix(self.ni, self.nh)\n",
    "        self.wo = makeMatrix(self.nh, self.no)\n",
    "        \n",
    "        # set them to random vaules\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                self.wi[i][j] = rand(-1, 1)\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                self.wo[j][k] = rand(-1, 1)\n",
    "\n",
    "        # last change in weights for momentum   \n",
    "        self.ci = makeMatrix(self.ni, self.nh)\n",
    "        self.co = makeMatrix(self.nh, self.no)\n",
    "\n",
    "\n",
    "    def update(self, inputs):\n",
    "        if len(inputs) != self.ni-1:\n",
    "            raise (ValueError, 'wrong number of inputs')\n",
    "\n",
    "        # input activations\n",
    "        for i in range(self.ni - 1):\n",
    "            self.ai[i] = inputs[i]\n",
    "\n",
    "        # hidden activations\n",
    "        for j in range(self.nh - 1):\n",
    "            total = 0.0\n",
    "            for i in range(self.ni):\n",
    "                total += self.ai[i] * self.wi[i][j]\n",
    "            self.ah[j] = sigmoid(total)\n",
    "\n",
    "        # output activations\n",
    "        for k in range(self.no):\n",
    "            total = 0.0\n",
    "            for j in range(self.nh):\n",
    "                total += self.ah[j] * self.wo[j][k]\n",
    "            self.ao[k] = total\n",
    "            if not self.regression:\n",
    "                self.ao[k] = sigmoid(total)\n",
    "            \n",
    "        \n",
    "        return self.ao[:]\n",
    "\n",
    "\n",
    "    def backPropagate(self, targets, N, M):\n",
    "        if len(targets) != self.no:\n",
    "            raise (ValueError, 'wrong number of target values')\n",
    "\n",
    "        # calculate error terms for output\n",
    "        output_deltas = [0.0] * self.no\n",
    "        for k in range(self.no):\n",
    "            output_deltas[k] = targets[k] - self.ao[k]\n",
    "            if not self.regression:\n",
    "                output_deltas[k] = dsigmoid(self.ao[k]) * output_deltas[k]\n",
    "\n",
    "        \n",
    "        # calculate error terms for hidden\n",
    "        hidden_deltas = [0.0] * self.nh\n",
    "        for j in range(self.nh):\n",
    "            error = 0.0\n",
    "            for k in range(self.no):\n",
    "                error += output_deltas[k]*self.wo[j][k]\n",
    "            hidden_deltas[j] = dsigmoid(self.ah[j]) * error\n",
    "\n",
    "        # update output weights\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                change = output_deltas[k]*self.ah[j]\n",
    "                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]\n",
    "                self.co[j][k] = change\n",
    "\n",
    "        # update input weights\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                change = hidden_deltas[j]*self.ai[i]\n",
    "                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\n",
    "                self.ci[i][j] = change\n",
    "\n",
    "        # calculate error\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error += 0.5*((targets[k]-self.ao[k])**2)\n",
    "        return error\n",
    "\n",
    "\n",
    "    def test(self, patterns, verbose = False):\n",
    "        tmp = []\n",
    "        for p in patterns:\n",
    "            if verbose:\n",
    "                print (p[0], '->', self.update(p[0]))\n",
    "            tmp.append(self.update(p[0]))\n",
    "\n",
    "        return tmp\n",
    "\n",
    "        \n",
    "    def weights(self):\n",
    "        print ('Input weights:')\n",
    "        for i in range(self.ni):\n",
    "            print (self.wi[i])\n",
    "        print\n",
    "        print ('Output weights:')\n",
    "        for j in range(self.nh):\n",
    "            print (self.wo[j])\n",
    "\n",
    "    def train(self, patterns, iterations=1000, N=0.5, M=0.1, verbose = False):\n",
    "        \"\"\"Train the neural network.  \n",
    "        \n",
    "        N is the learning rate.\n",
    "        M is the momentum factor.\n",
    "        \"\"\"\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                self.update(p[0])\n",
    "                tmp = self.backPropagate(p[1], N, M)\n",
    "                error += tmp\n",
    "                \n",
    "            if i % 100 == 0:\n",
    "                print ('error %-14f' % error)\n",
    "            \n",
    "\n",
    "def demoRegression():\n",
    "    data = []\n",
    "    inputs = []\n",
    "    actual = []\n",
    "\n",
    "    domain = [-1, 1]\n",
    "    steps = 50\n",
    "    stepsize = (domain[1] - domain[0]) / ((steps - 1)*1.0)\n",
    "\n",
    "    #Teach the network the function y = x**2\n",
    "    for i in range(steps):\n",
    "        x = domain[0] + stepsize * i\n",
    "        y = x**2\n",
    "        \n",
    "        data.append([[x], [y]])\n",
    "        inputs.append(x)\n",
    "        actual.append(y)\n",
    "        \n",
    "    n = NN(1, 4, 1, regression = True)\n",
    "    \n",
    "    #Train and test the nural network.\n",
    "    n.train(data, 1000, 0.2, 0.1, False)\n",
    "    outputs = n.test(data, verbose = True)\n",
    "    \n",
    "    #Plot the function.\n",
    "    try:\n",
    "        plot(inputs, outputs, actual)\n",
    "        print (\"Press a key to quit.\")\n",
    "        value = raw_input()\n",
    "    except:\n",
    "        print (\"Must have matplotlib to plot.\")\n",
    "\n",
    "\n",
    "def demoClassification():\n",
    "    # Teach network XOR function\n",
    "    pat = [\n",
    "        [[0,0], [0]],\n",
    "        [[0,1], [1]],\n",
    "        [[1,0], [1]],\n",
    "        [[1,1], [0]]\n",
    "    ]\n",
    "\n",
    "    # create a network with two input, two hidden, and one output nodes\n",
    "    n = NN(2, 2, 1, regression = False)\n",
    "\n",
    "    # train it with some patterns then test it.\n",
    "    n.train(pat, 1000, 0.5, 0.2)\n",
    "    n.test(pat, verbose = True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #demoRegression()\n",
    "    demoClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
